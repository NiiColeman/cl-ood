
--- Run 1/3 ---

Running LoRA with TIES experiment
Test domain: sketch
Train domains: ['photo', 'art_painting', 'cartoon']
Baseline Epoch 1/10, Train Loss: 1.5390, Train Accuracy: 43.43%
Val Loss: 1.1778, Val Accuracy: 59.98%
Baseline Epoch 2/10, Train Loss: 0.8216, Train Accuracy: 73.36%
Val Loss: 0.9010, Val Accuracy: 63.67%
Baseline Epoch 3/10, Train Loss: 0.4141, Train Accuracy: 86.13%
Val Loss: 0.8006, Val Accuracy: 67.68%
Baseline Epoch 4/10, Train Loss: 0.2074, Train Accuracy: 94.16%
Val Loss: 0.8774, Val Accuracy: 68.17%
Baseline Epoch 5/10, Train Loss: 0.0818, Train Accuracy: 99.27%
Val Loss: 0.8605, Val Accuracy: 70.92%
Baseline Epoch 6/10, Train Loss: 0.0390, Train Accuracy: 99.64%
Val Loss: 0.8230, Val Accuracy: 72.34%
Baseline Epoch 7/10, Train Loss: 0.0214, Train Accuracy: 100.00%
Val Loss: 0.8441, Val Accuracy: 72.14%
Baseline Epoch 8/10, Train Loss: 0.0074, Train Accuracy: 100.00%
Val Loss: 0.8638, Val Accuracy: 72.71%
Baseline Epoch 9/10, Train Loss: 0.0055, Train Accuracy: 100.00%
Val Loss: 0.8410, Val Accuracy: 73.64%
Baseline Epoch 10/10, Train Loss: 0.0034, Train Accuracy: 100.00%
Val Loss: 0.8586, Val Accuracy: 73.76%
Baseline Model - Final Test Accuracy: 73.76%

Training LoRA adapter for domain: photo

Training LoRA adapter for domain: photo
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.6563, Train Accuracy: 37.95%
Val Loss: 0.9636, Val Accuracy: 70.96%
Epoch 2/10, Train Loss: 0.5468, Train Accuracy: 89.00%
Val Loss: 0.1679, Val Accuracy: 97.01%
Epoch 3/10, Train Loss: 0.0753, Train Accuracy: 98.80%
Val Loss: 0.0609, Val Accuracy: 98.50%
Epoch 4/10, Train Loss: 0.0169, Train Accuracy: 99.85%
Val Loss: 0.0310, Val Accuracy: 99.10%
Epoch 5/10, Train Loss: 0.0070, Train Accuracy: 100.00%
Val Loss: 0.0208, Val Accuracy: 99.10%
Epoch 6/10, Train Loss: 0.0034, Train Accuracy: 100.00%
Val Loss: 0.0174, Val Accuracy: 99.10%
Epoch 7/10, Train Loss: 0.0021, Train Accuracy: 100.00%
Val Loss: 0.0154, Val Accuracy: 99.10%
Epoch 8/10, Train Loss: 0.0016, Train Accuracy: 100.00%
Val Loss: 0.0147, Val Accuracy: 99.10%
Epoch 9/10, Train Loss: 0.0012, Train Accuracy: 100.00%
Val Loss: 0.0137, Val Accuracy: 99.10%
Epoch 10/10, Train Loss: 0.0010, Train Accuracy: 100.00%
Val Loss: 0.0130, Val Accuracy: 99.10%
Best validation accuracy for photo: 99.10%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])
photo adapter trained. Validation Accuracy: 99.10%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])

Training LoRA adapter for domain: art_painting

Training LoRA adapter for domain: art_painting
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.7276, Train Accuracy: 35.04%
Val Loss: 1.1595, Val Accuracy: 64.39%
Epoch 2/10, Train Loss: 0.6220, Train Accuracy: 84.07%
Val Loss: 0.2767, Val Accuracy: 93.90%
Epoch 3/10, Train Loss: 0.1504, Train Accuracy: 96.52%
Val Loss: 0.1905, Val Accuracy: 94.88%
Epoch 4/10, Train Loss: 0.0587, Train Accuracy: 98.90%
Val Loss: 0.1350, Val Accuracy: 96.34%
Epoch 5/10, Train Loss: 0.0214, Train Accuracy: 99.94%
Val Loss: 0.1137, Val Accuracy: 97.07%
Epoch 6/10, Train Loss: 0.0102, Train Accuracy: 100.00%
Val Loss: 0.1072, Val Accuracy: 96.10%
Epoch 7/10, Train Loss: 0.0063, Train Accuracy: 100.00%
Val Loss: 0.1048, Val Accuracy: 97.07%
Epoch 8/10, Train Loss: 0.0044, Train Accuracy: 100.00%
Val Loss: 0.1024, Val Accuracy: 97.56%
Epoch 9/10, Train Loss: 0.0033, Train Accuracy: 100.00%
Val Loss: 0.1042, Val Accuracy: 97.56%
Epoch 10/10, Train Loss: 0.0025, Train Accuracy: 100.00%
Val Loss: 0.1036, Val Accuracy: 97.80%
Best validation accuracy for art_painting: 97.80%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])
art_painting adapter trained. Validation Accuracy: 97.80%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])

Training LoRA adapter for domain: cartoon

Training LoRA adapter for domain: cartoon
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.7078, Train Accuracy: 34.29%
Val Loss: 1.1594, Val Accuracy: 58.21%
Epoch 2/10, Train Loss: 0.6818, Train Accuracy: 78.72%
Val Loss: 0.4560, Val Accuracy: 84.86%
Epoch 3/10, Train Loss: 0.2613, Train Accuracy: 91.89%
Val Loss: 0.2805, Val Accuracy: 92.11%
Epoch 4/10, Train Loss: 0.1325, Train Accuracy: 96.00%
Val Loss: 0.1937, Val Accuracy: 93.82%
Epoch 5/10, Train Loss: 0.0735, Train Accuracy: 98.24%
Val Loss: 0.1946, Val Accuracy: 94.24%
Epoch 6/10, Train Loss: 0.0378, Train Accuracy: 99.41%
Val Loss: 0.1671, Val Accuracy: 94.88%
Epoch 7/10, Train Loss: 0.0235, Train Accuracy: 99.79%
Val Loss: 0.1815, Val Accuracy: 95.31%
Epoch 8/10, Train Loss: 0.0123, Train Accuracy: 100.00%
Val Loss: 0.1559, Val Accuracy: 95.31%
Epoch 9/10, Train Loss: 0.0078, Train Accuracy: 100.00%
Val Loss: 0.1448, Val Accuracy: 96.16%
Epoch 10/10, Train Loss: 0.0055, Train Accuracy: 100.00%
Val Loss: 0.1469, Val Accuracy: 95.74%
Best validation accuracy for cartoon: 96.16%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])
cartoon adapter trained. Validation Accuracy: 96.16%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])

Number of trained LoRA adapters: 3

Evaluating individual LoRA adapters on the test domain:
Created LoRA model: <class 'peft.peft_model.PeftModel'>
photo adapter - Test Loss: 2.0505, Test Accuracy: 32.85%
Created LoRA model: <class 'peft.peft_model.PeftModel'>
art_painting adapter - Test Loss: 1.3332, Test Accuracy: 50.16%
Created LoRA model: <class 'peft.peft_model.PeftModel'>
cartoon adapter - Test Loss: 2.4264, Test Accuracy: 35.12%

Training coefficients for adapter merging
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Coefficient gradients: tensor([-2.7415e-09, -2.7415e-09, -2.7415e-09], device='cuda:0')
Coefficient gradients: tensor([-3.4255e-08, -3.4255e-08, -3.4255e-08], device='cuda:0')
Coefficient gradients: tensor([-4.5627e-09, -4.5627e-09, -4.5627e-09], device='cuda:0')
Coefficient gradients: tensor([3.1091e-09, 3.1091e-09, 3.1091e-09], device='cuda:0')
Coefficient gradients: tensor([-1.7911e-08, -1.7911e-08, -1.7911e-08], device='cuda:0')
Coefficient gradients: tensor([-1.5272e-08, -1.5272e-08, -1.5272e-08], device='cuda:0')
Coefficient gradients: tensor([9.1388e-09, 9.1388e-09, 9.1388e-09], device='cuda:0')
Coefficient gradients: tensor([-1.2102e-08, -1.2102e-08, -1.2102e-08], device='cuda:0')
Coefficient gradients: tensor([-1.7088e-08, -1.7088e-08, -1.7088e-08], device='cuda:0')
Epoch 1, Loss: 2.2508, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-3.2424e-08, -3.2424e-08, -3.2424e-08], device='cuda:0')
Coefficient gradients: tensor([-1.4506e-08, -1.4506e-08, -1.4506e-08], device='cuda:0')
Coefficient gradients: tensor([-1.6663e-08, -1.6663e-08, -1.6663e-08], device='cuda:0')
Coefficient gradients: tensor([-1.9320e-09, -1.9320e-09, -1.9320e-09], device='cuda:0')
Coefficient gradients: tensor([8.7886e-09, 8.7886e-09, 8.7886e-09], device='cuda:0')
Coefficient gradients: tensor([-3.8805e-08, -3.8805e-08, -3.8805e-08], device='cuda:0')
Coefficient gradients: tensor([-2.0151e-08, -2.0151e-08, -2.0151e-08], device='cuda:0')
Coefficient gradients: tensor([-1.1762e-08, -1.1762e-08, -1.1762e-08], device='cuda:0')
Coefficient gradients: tensor([-1.1423e-08, -1.1423e-08, -1.1423e-08], device='cuda:0')
Epoch 2, Loss: 2.2250, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-3.4570e-08, -3.4570e-08, -3.4570e-08], device='cuda:0')
Coefficient gradients: tensor([6.8181e-09, 6.8181e-09, 6.8181e-09], device='cuda:0')
Coefficient gradients: tensor([-1.5164e-08, -1.5164e-08, -1.5164e-08], device='cuda:0')
Coefficient gradients: tensor([-1.9581e-08, -1.9581e-08, -1.9581e-08], device='cuda:0')
Coefficient gradients: tensor([-1.8467e-08, -1.8467e-08, -1.8467e-08], device='cuda:0')
Coefficient gradients: tensor([9.2204e-09, 9.2204e-09, 9.2204e-09], device='cuda:0')
Coefficient gradients: tensor([-1.4414e-08, -1.4414e-08, -1.4414e-08], device='cuda:0')
Coefficient gradients: tensor([-1.0914e-08, -1.0914e-08, -1.0914e-08], device='cuda:0')
Coefficient gradients: tensor([7.4131e-09, 7.4131e-09, 7.4131e-09], device='cuda:0')
Epoch 3, Loss: 2.2321, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-3.5258e-09, -3.5258e-09, -3.5258e-09], device='cuda:0')
Coefficient gradients: tensor([-9.3459e-09, -9.3459e-09, -9.3459e-09], device='cuda:0')
Coefficient gradients: tensor([3.2936e-09, 3.2936e-09, 3.2936e-09], device='cuda:0')
Coefficient gradients: tensor([9.3245e-09, 9.3245e-09, 9.3245e-09], device='cuda:0')
Coefficient gradients: tensor([-1.3557e-08, -1.3557e-08, -1.3557e-08], device='cuda:0')
Coefficient gradients: tensor([-1.7504e-08, -1.7504e-08, -1.7504e-08], device='cuda:0')
Coefficient gradients: tensor([-1.7396e-08, -1.7396e-08, -1.7396e-08], device='cuda:0')
Coefficient gradients: tensor([-1.7241e-08, -1.7241e-08, -1.7241e-08], device='cuda:0')
Coefficient gradients: tensor([-4.8615e-09, -4.8615e-09, -4.8615e-09], device='cuda:0')
Epoch 4, Loss: 2.2433, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-4.2920e-09, -4.2920e-09, -4.2920e-09], device='cuda:0')
Coefficient gradients: tensor([-9.0677e-09, -9.0677e-09, -9.0677e-09], device='cuda:0')
Coefficient gradients: tensor([-9.7458e-11, -9.7458e-11, -9.7458e-11], device='cuda:0')
Coefficient gradients: tensor([-2.5570e-09, -2.5570e-09, -2.5570e-09], device='cuda:0')
Coefficient gradients: tensor([-1.5553e-08, -1.5553e-08, -1.5553e-08], device='cuda:0')
Coefficient gradients: tensor([1.9652e-08, 1.9652e-08, 1.9652e-08], device='cuda:0')
Coefficient gradients: tensor([-1.5877e-08, -1.5877e-08, -1.5877e-08], device='cuda:0')
Coefficient gradients: tensor([8.7362e-09, 8.7362e-09, 8.7362e-09], device='cuda:0')
Coefficient gradients: tensor([-9.8625e-09, -9.8625e-09, -9.8625e-09], device='cuda:0')
Epoch 5, Loss: 2.2186, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-3.7688e-08, -3.7688e-08, -3.7688e-08], device='cuda:0')
Coefficient gradients: tensor([-3.6915e-08, -3.6915e-08, -3.6915e-08], device='cuda:0')
Coefficient gradients: tensor([-4.5674e-09, -4.5674e-09, -4.5674e-09], device='cuda:0')
Coefficient gradients: tensor([-2.1749e-09, -2.1749e-09, -2.1749e-09], device='cuda:0')
Coefficient gradients: tensor([-8.1259e-09, -8.1259e-09, -8.1259e-09], device='cuda:0')
Coefficient gradients: tensor([5.8960e-09, 5.8960e-09, 5.8960e-09], device='cuda:0')
Coefficient gradients: tensor([1.5373e-09, 1.5373e-09, 1.5373e-09], device='cuda:0')
Coefficient gradients: tensor([-1.6836e-09, -1.6836e-09, -1.6836e-09], device='cuda:0')
Coefficient gradients: tensor([1.3429e-09, 1.3429e-09, 1.3429e-09], device='cuda:0')
Epoch 6, Loss: 2.2648, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-1.5740e-08, -1.5740e-08, -1.5740e-08], device='cuda:0')
Coefficient gradients: tensor([-9.5693e-09, -9.5693e-09, -9.5693e-09], device='cuda:0')
Coefficient gradients: tensor([-4.8289e-09, -4.8289e-09, -4.8289e-09], device='cuda:0')
Coefficient gradients: tensor([-2.4399e-08, -2.4399e-08, -2.4399e-08], device='cuda:0')
Coefficient gradients: tensor([2.6792e-09, 2.6792e-09, 2.6792e-09], device='cuda:0')
Coefficient gradients: tensor([-1.8082e-08, -1.8082e-08, -1.8082e-08], device='cuda:0')
Coefficient gradients: tensor([-1.5286e-08, -1.5286e-08, -1.5286e-08], device='cuda:0')
Coefficient gradients: tensor([2.8815e-09, 2.8815e-09, 2.8815e-09], device='cuda:0')
Coefficient gradients: tensor([-1.1058e-09, -1.1058e-09, -1.1058e-09], device='cuda:0')
Epoch 7, Loss: 2.2733, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-1.8234e-08, -1.8234e-08, -1.8234e-08], device='cuda:0')
Coefficient gradients: tensor([-1.1519e-08, -1.1519e-08, -1.1519e-08], device='cuda:0')
Coefficient gradients: tensor([-1.6506e-08, -1.6506e-08, -1.6506e-08], device='cuda:0')
Coefficient gradients: tensor([-1.6880e-08, -1.6880e-08, -1.6880e-08], device='cuda:0')
Coefficient gradients: tensor([-7.9468e-09, -7.9468e-09, -7.9468e-09], device='cuda:0')
Coefficient gradients: tensor([7.3614e-09, 7.3614e-09, 7.3614e-09], device='cuda:0')
Coefficient gradients: tensor([4.9902e-09, 4.9902e-09, 4.9902e-09], device='cuda:0')
Coefficient gradients: tensor([-1.4985e-08, -1.4985e-08, -1.4985e-08], device='cuda:0')
Coefficient gradients: tensor([-3.8517e-08, -3.8517e-08, -3.8517e-08], device='cuda:0')
Epoch 8, Loss: 2.2596, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-1.5339e-08, -1.5339e-08, -1.5339e-08], device='cuda:0')
Coefficient gradients: tensor([-2.2313e-09, -2.2313e-09, -2.2313e-09], device='cuda:0')
Coefficient gradients: tensor([-1.6225e-08, -1.6225e-08, -1.6225e-08], device='cuda:0')
Coefficient gradients: tensor([7.0598e-10, 7.0598e-10, 7.0598e-10], device='cuda:0')
Coefficient gradients: tensor([9.7538e-09, 9.7538e-09, 9.7538e-09], device='cuda:0')
Coefficient gradients: tensor([5.9619e-09, 5.9619e-09, 5.9619e-09], device='cuda:0')
Coefficient gradients: tensor([-4.2532e-09, -4.2532e-09, -4.2532e-09], device='cuda:0')
Coefficient gradients: tensor([-3.1583e-08, -3.1583e-08, -3.1583e-08], device='cuda:0')
Coefficient gradients: tensor([-1.9787e-08, -1.9787e-08, -1.9787e-08], device='cuda:0')
Epoch 9, Loss: 2.2620, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Coefficient gradients: tensor([-1.3787e-08, -1.3787e-08, -1.3787e-08], device='cuda:0')
Coefficient gradients: tensor([-1.1131e-08, -1.1131e-08, -1.1131e-08], device='cuda:0')
Coefficient gradients: tensor([-1.3074e-08, -1.3074e-08, -1.3074e-08], device='cuda:0')
Coefficient gradients: tensor([1.9956e-09, 1.9956e-09, 1.9956e-09], device='cuda:0')
Coefficient gradients: tensor([-1.5869e-08, -1.5869e-08, -1.5869e-08], device='cuda:0')
Coefficient gradients: tensor([-1.4469e-08, -1.4469e-08, -1.4469e-08], device='cuda:0')
Coefficient gradients: tensor([-3.7532e-08, -3.7532e-08, -3.7532e-08], device='cuda:0')
Coefficient gradients: tensor([-2.9913e-08, -2.9913e-08, -2.9913e-08], device='cuda:0')
Coefficient gradients: tensor([-1.7839e-08, -1.7839e-08, -1.7839e-08], device='cuda:0')
Epoch 10, Loss: 2.2617, Accuracy: 38.32%
Coefficients: [0.33333334 0.33333334 0.33333334]
Best coefficient accuracy: 38.32%
Final best coefficients: [0.33333334 0.33333334 0.33333334]

Merged model with TIES - Test Loss: 1.6858, Test Accuracy: 34.47%

--- Run 2/3 ---

Running LoRA with TIES experiment
Test domain: sketch
Train domains: ['photo', 'art_painting', 'cartoon']
Baseline Epoch 1/10, Train Loss: 1.5390, Train Accuracy: 43.43%
Val Loss: 1.1778, Val Accuracy: 59.98%
Baseline Epoch 2/10, Train Loss: 0.8216, Train Accuracy: 73.36%
Val Loss: 0.9010, Val Accuracy: 63.67%
Baseline Epoch 3/10, Train Loss: 0.4141, Train Accuracy: 86.13%
Val Loss: 0.8006, Val Accuracy: 67.68%
Baseline Epoch 4/10, Train Loss: 0.2074, Train Accuracy: 94.16%
Val Loss: 0.8774, Val Accuracy: 68.17%
Baseline Epoch 5/10, Train Loss: 0.0818, Train Accuracy: 99.27%
Val Loss: 0.8605, Val Accuracy: 70.92%
Baseline Epoch 6/10, Train Loss: 0.0390, Train Accuracy: 99.64%
Val Loss: 0.8230, Val Accuracy: 72.34%
Baseline Epoch 7/10, Train Loss: 0.0214, Train Accuracy: 100.00%
Val Loss: 0.8441, Val Accuracy: 72.14%
Baseline Epoch 8/10, Train Loss: 0.0074, Train Accuracy: 100.00%
Val Loss: 0.8638, Val Accuracy: 72.71%
Baseline Epoch 9/10, Train Loss: 0.0055, Train Accuracy: 100.00%
Val Loss: 0.8410, Val Accuracy: 73.64%
Baseline Epoch 10/10, Train Loss: 0.0034, Train Accuracy: 100.00%
Val Loss: 0.8586, Val Accuracy: 73.76%
Baseline Model - Final Test Accuracy: 73.76%

Training LoRA adapter for domain: photo

Training LoRA adapter for domain: photo
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.6564, Train Accuracy: 37.95%
Val Loss: 0.9624, Val Accuracy: 70.96%
Epoch 2/10, Train Loss: 0.5475, Train Accuracy: 89.00%
Val Loss: 0.1706, Val Accuracy: 97.01%
Epoch 3/10, Train Loss: 0.0763, Train Accuracy: 98.80%
Val Loss: 0.0630, Val Accuracy: 98.50%
Epoch 4/10, Train Loss: 0.0171, Train Accuracy: 99.78%
Val Loss: 0.0319, Val Accuracy: 98.80%
Epoch 5/10, Train Loss: 0.0072, Train Accuracy: 100.00%
Val Loss: 0.0212, Val Accuracy: 99.10%
Epoch 6/10, Train Loss: 0.0034, Train Accuracy: 100.00%
Val Loss: 0.0178, Val Accuracy: 99.10%
Epoch 7/10, Train Loss: 0.0022, Train Accuracy: 100.00%
Val Loss: 0.0157, Val Accuracy: 99.10%
Epoch 8/10, Train Loss: 0.0016, Train Accuracy: 100.00%
Val Loss: 0.0151, Val Accuracy: 99.10%
Epoch 9/10, Train Loss: 0.0012, Train Accuracy: 100.00%
Val Loss: 0.0141, Val Accuracy: 99.10%
Epoch 10/10, Train Loss: 0.0010, Train Accuracy: 100.00%
Val Loss: 0.0136, Val Accuracy: 99.10%
Best validation accuracy for photo: 99.10%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])
photo adapter trained. Validation Accuracy: 99.10%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])

Training LoRA adapter for domain: art_painting

Training LoRA adapter for domain: art_painting
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.7276, Train Accuracy: 35.04%
Val Loss: 1.1591, Val Accuracy: 64.39%
Epoch 2/10, Train Loss: 0.6215, Train Accuracy: 84.13%
Val Loss: 0.2751, Val Accuracy: 93.90%
Epoch 3/10, Train Loss: 0.1494, Train Accuracy: 96.58%
Val Loss: 0.1974, Val Accuracy: 94.63%
Epoch 4/10, Train Loss: 0.0585, Train Accuracy: 98.84%
Val Loss: 0.1373, Val Accuracy: 96.34%
Epoch 5/10, Train Loss: 0.0213, Train Accuracy: 100.00%
Val Loss: 0.1146, Val Accuracy: 97.07%
Epoch 6/10, Train Loss: 0.0101, Train Accuracy: 100.00%
Val Loss: 0.1089, Val Accuracy: 96.10%
Epoch 7/10, Train Loss: 0.0063, Train Accuracy: 100.00%
Val Loss: 0.1061, Val Accuracy: 96.83%
Epoch 8/10, Train Loss: 0.0043, Train Accuracy: 100.00%
Val Loss: 0.1035, Val Accuracy: 96.83%
Epoch 9/10, Train Loss: 0.0033, Train Accuracy: 100.00%
Val Loss: 0.1056, Val Accuracy: 97.32%
Epoch 10/10, Train Loss: 0.0025, Train Accuracy: 100.00%
Val Loss: 0.1045, Val Accuracy: 97.56%
Best validation accuracy for art_painting: 97.56%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])
art_painting adapter trained. Validation Accuracy: 97.56%
LoRA state keys: dict_keys(['base_model.model.blocks.0.attn.qkv.lora_A.weight', 'base_model.model.blocks.0.attn.qkv.lora_B.weight', 'base_model.model.blocks.1.attn.qkv.lora_A.weight', 'base_model.model.blocks.1.attn.qkv.lora_B.weight', 'base_model.model.blocks.2.attn.qkv.lora_A.weight', 'base_model.model.blocks.2.attn.qkv.lora_B.weight', 'base_model.model.blocks.3.attn.qkv.lora_A.weight', 'base_model.model.blocks.3.attn.qkv.lora_B.weight', 'base_model.model.blocks.4.attn.qkv.lora_A.weight', 'base_model.model.blocks.4.attn.qkv.lora_B.weight', 'base_model.model.blocks.5.attn.qkv.lora_A.weight', 'base_model.model.blocks.5.attn.qkv.lora_B.weight', 'base_model.model.blocks.6.attn.qkv.lora_A.weight', 'base_model.model.blocks.6.attn.qkv.lora_B.weight', 'base_model.model.blocks.7.attn.qkv.lora_A.weight', 'base_model.model.blocks.7.attn.qkv.lora_B.weight', 'base_model.model.blocks.8.attn.qkv.lora_A.weight', 'base_model.model.blocks.8.attn.qkv.lora_B.weight', 'base_model.model.blocks.9.attn.qkv.lora_A.weight', 'base_model.model.blocks.9.attn.qkv.lora_B.weight', 'base_model.model.blocks.10.attn.qkv.lora_A.weight', 'base_model.model.blocks.10.attn.qkv.lora_B.weight', 'base_model.model.blocks.11.attn.qkv.lora_A.weight', 'base_model.model.blocks.11.attn.qkv.lora_B.weight'])

Training LoRA adapter for domain: cartoon

Training LoRA adapter for domain: cartoon
Created LoRA model: <class 'peft.peft_model.PeftModel'>
Epoch 1/10, Train Loss: 1.7078, Train Accuracy: 34.29%
Val Loss: 1.1593, Val Accuracy: 58.21%
Epoch 2/10, Train Loss: 0.6819, Train Accuracy: 78.61%
Val Loss: 0.4561, Val Accuracy: 84.86%
Epoch 3/10, Train Loss: 0.2624, Train Accuracy: 91.79%
Val Loss: 0.2797, Val Accuracy: 91.90%
Epoch 4/10, Train Loss: 0.1326, Train Accuracy: 96.16%
Val Loss: 0.1969, Val Accuracy: 93.82%
Epoch 5/10, Train Loss: 0.0751, Train Accuracy: 98.13%
Val Loss: 0.1964, Val Accuracy: 94.46%
Epoch 6/10, Train Loss: 0.0389, Train Accuracy: 99.52%
Val Loss: 0.1617, Val Accuracy: 95.10%
